{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing drivers...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitializing drivers...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m ws_driver \u001b[39m=\u001b[39m CkipWordSegmenter(model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39malbert-base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m pos_driver \u001b[39m=\u001b[39m CkipPosTagger(model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39malbert-base\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitializing drivers... done\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\2_DataScience\\04_google_map_crawler\\venv\\lib\\site-packages\\ckip_transformers\\nlp\\driver.py:153\u001b[0m, in \u001b[0;36mCkipPosTagger.__init__\u001b[1;34m(self, model, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    148\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    149\u001b[0m     model: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbert-base\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    150\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    151\u001b[0m ):\n\u001b[0;32m    152\u001b[0m     model_name \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_model_name(model))\n\u001b[1;32m--> 153\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(model_name\u001b[39m=\u001b[39mmodel_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\2_DataScience\\04_google_map_crawler\\venv\\lib\\site-packages\\ckip_transformers\\nlp\\util.py:67\u001b[0m, in \u001b[0;36mCkipTokenClassification.__init__\u001b[1;34m(self, model_name, tokenizer_name, device)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     60\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     61\u001b[0m     model_name: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     device: Union[\u001b[39mint\u001b[39m, torch\u001b[39m.\u001b[39mdevice] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     65\u001b[0m ):\n\u001b[0;32m     66\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AutoModelForTokenClassification\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m---> 67\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m BertTokenizerFast\u001b[39m.\u001b[39;49mfrom_pretrained(tokenizer_name \u001b[39mor\u001b[39;49;00m model_name)\n\u001b[0;32m     69\u001b[0m     \u001b[39m# Allow passing a customized torch.device.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, torch\u001b[39m.\u001b[39mdevice):\n",
      "File \u001b[1;32mc:\\2_DataScience\\04_google_map_crawler\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1854\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1851\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1852\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1854\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained(\n\u001b[0;32m   1855\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   1856\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   1857\u001b[0m     init_configuration,\n\u001b[0;32m   1858\u001b[0m     \u001b[39m*\u001b[39minit_inputs,\n\u001b[0;32m   1859\u001b[0m     token\u001b[39m=\u001b[39mtoken,\n\u001b[0;32m   1860\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1861\u001b[0m     local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1862\u001b[0m     _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   1863\u001b[0m     _is_local\u001b[39m=\u001b[39mis_local,\n\u001b[0;32m   1864\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1865\u001b[0m )\n",
      "File \u001b[1;32mc:\\2_DataScience\\04_google_map_crawler\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2032\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2030\u001b[0m special_tokens_map_file \u001b[39m=\u001b[39m resolved_vocab_files\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mspecial_tokens_map_file\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   2031\u001b[0m \u001b[39mif\u001b[39;00m special_tokens_map_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2032\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(special_tokens_map_file, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m special_tokens_map_handle:\n\u001b[0;32m   2033\u001b[0m         special_tokens_map \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(special_tokens_map_handle)\n\u001b[0;32m   2034\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m special_tokens_map\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32mC:\\Python310\\lib\\codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[0;32m    304\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[39m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[39m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m    byte sequences.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    310\u001b[0m         IncrementalDecoder\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, errors)\n\u001b[0;32m    311\u001b[0m         \u001b[39m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Initializing drivers...\")\n",
    "ws_driver = CkipWordSegmenter(model=\"albert-base\")\n",
    "pos_driver = CkipPosTagger(model=\"albert-base\")\n",
    "print(\"Initializing drivers... done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(article):\n",
    "    if not article:\n",
    "        return \"\"\n",
    "    # 斷詞\n",
    "    ws = ws_driver([article])\n",
    "    # 詞性標注\n",
    "    pos = pos_driver(ws)\n",
    "\n",
    "    # 只留下名詞和動詞，並去掉特定詞性、一個字詞\n",
    "    short_sentence = []\n",
    "    stop_pos = set([]) #不使用停用詞\n",
    "    for sentence_ws, sentence_pos in zip(ws, pos):\n",
    "        for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
    "            is_N_or_V = word_pos.startswith(\"V\") or word_pos.startswith(\"N\")\n",
    "            is_not_stop_pos = word_pos not in stop_pos\n",
    "            is_not_one_charactor = len(word_ws) > 1\n",
    "            if is_N_or_V and is_not_stop_pos and is_not_one_charactor:\n",
    "                short_sentence.append(word_ws)\n",
    "    # 回傳斷詞後的結果\n",
    "    return \" \".join(short_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "directory = './data/csv/'\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    dataset = pd.read_csv('./data/csv/{}'.format(file))\n",
    "    dataset[\"content\"].fillna(\"\", inplace=True)\n",
    "    content = dataset[\"content\"].apply(lambda x: re.sub(\"[^\\w\\s\\(\\)\\*\\+\\?\\.\\|]\", \"\", str(x)))\n",
    "    seg_result = content.apply(process_article)\n",
    "\n",
    "    seg_result.to_csv('./data/seg_result/{}'.format(file), index=False)\n",
    "\n",
    "    print(f\"{count}:{file}已輸出\")\n",
    "    count = count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "directory = './data/csv/'\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    article = []\n",
    "    dataset = pd.read_csv('./data/csv/{}'.format(file))\n",
    "    dataset[\"content\"].fillna(\"\", inplace=True)\n",
    "    content = dataset[\"content\"].apply(lambda x: re.sub(\"[^\\w\\s\\(\\)\\*\\+\\?\\.\\|]\", \"\", str(x)))\n",
    "    seg_result = content.apply(process_article)\n",
    "    \n",
    "    for word in seg_result:\n",
    "        if word == \"\": continue\n",
    "        for ws in word.split(' '):\n",
    "            if ws != \"\":\n",
    "                article.append(ws)\n",
    "    word_list.append(article)\n",
    "\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(word_list, window=10, min_count=1, workers=6)\n",
    "vocab = model.wv.index_to_key\n",
    "vectors = model.wv[vocab]\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/word2vec_gensim.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
